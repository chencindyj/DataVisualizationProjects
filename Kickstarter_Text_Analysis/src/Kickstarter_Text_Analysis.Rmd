---
title: "Kickstarter Fundraising Text Analysis"
author: "Cindy Chen"
date: "3/26/2022"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(tidymodels)
library(tidytext)
library(tm)
library(forcats)
library(gghighlight)
library(viridisLite)
library(scales)
library(wordcloud)
library(plotrix)
library(ggthemes)
library(quanteda)
library(quanteda.textstats)
library(XML)
library(rvest)
library(leaflet)
library(ggmap)
library(RColorBrewer)

set.seed(12345)
```

## Loading & Preparing Data

```{r cars}
kickstarter <- read.csv("kickstarter_projects_2021-05.csv")
# str(kickstarter)
```

## 1. Identifying Successful Projects

### a) Success by Category

I will visualize success of projects based on the Achievement Ratio and the Pledged Amount.

```{r}
# summarize top campaigns by defined metrics and by category
successful_categories <- kickstarter %>%
  select(top_category, sub_category, converted_pledged_amount, pledged, goal) %>%
  mutate(achievement_ratio = pledged/goal * 100) %>%
  group_by(top_category) %>%
  summarize(avg_pledged = mean(converted_pledged_amount),
            mean_achievement_ratio = mean(achievement_ratio)) %>%
  arrange(desc(avg_pledged))

# Plot #1: successful major categories by pledged amount
successful_categories %>%
  mutate(avg_pledged = avg_pledged/1000) %>% #take the number in thousands
  ggplot() +
  geom_col(aes(x = fct_reorder(top_category, desc(avg_pledged)),
               y = avg_pledged, fill = top_category)) +
  scale_fill_viridis_d() +
  gghighlight(avg_pledged > 20) +
  labs(title = "Top Kickstarter Categories by Average $ Amt Pledged:\nTechnology, Design, and Games Campaigns Lead Fundraising",
       x = "Major Category",
       y = "Pledged $ in 000s (Converted)") +
  theme_classic() + 
  theme(legend.position = "none") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
```{r}
# Plot 2: achievement ratio by Sub-Category
# summarize top campaigns by defined metrics and by category

successful_categories %>%
  ungroup() %>%
  filter(top_category != "") %>% #I only want entries where a sub-category is given
  slice_head(n = 10) %>%
  ggplot() +
  geom_col(aes(x = fct_reorder(top_category, mean_achievement_ratio),
               y = mean_achievement_ratio, fill = top_category)) +
  coord_flip() +
  scale_fill_viridis_d() +
  gghighlight(mean_achievement_ratio > 500) +
  # guides(fill=guide_legend(title="Category")) +
  labs(title = "Top 10 Kickstarter Categories by Average Achievement Ratio:\nSix Categories Exceed Avg Goals by 500%",
       x = "Kickstarter Category",
       y = "Achievement Ratio (as % of Goal)") +
  theme_classic() + 
  theme(legend.position = "none")
```

**BONUS ONLY:**
b) Success by Location

```{r by_state}
total_projs_by_state <- kickstarter %>%
  select(location_state, state) %>%
  group_by(location_state) %>%
  count() %>%
  rename(total_projs = n)

successful_by_state <- kickstarter %>%
  select(location_state, state) %>%
  filter(state == "successful") %>%
  group_by(location_state) %>%
  count() %>%
  rename(successful_projs = n)
```

```{r pull_state_populations}

# read HTML table of state populations from wikipedia
html_file <- html_table(
  read_html(
    "http://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population"))

census_population <- html_file[[1]]

# transform the HTML table headings to usable column names
names(census_population) <- make.names(names(census_population))

# rename the column with the latest population values

names(census_population)[4] <- "latest_pop"

census_population_df <- census_population %>%
  as.data.frame() %>%
  select(State.or.territory, latest_pop) %>%
  filter(State.or.territory != "State or territory") %>%
  mutate(latest_pop = as.integer(gsub(",", "", latest_pop))) %>%
  # recode all the states
  mutate(st_code = case_when(State.or.territory == "California" ~ "CA",
                   State.or.territory == "New York" ~ "NY",
                   State.or.territory == "Nebraska" ~ "NE",
                   State.or.territory == "Alabama" ~ "AL",
                   State.or.territory == "Alaska" ~ "AK",
                   State.or.territory == "Florida" ~ "FL",
                   State.or.territory == "Georgia" ~ "GA",
                   State.or.territory == "Tennessee" ~ "TN",
                   State.or.territory == "Washington" ~ "WA",
                   State.or.territory == "Rhode Island" ~ "RI",
                   State.or.territory == "Arizona" ~ "AZ",
                   State.or.territory == "Indiana" ~ "IN",
                   State.or.territory == "Ohio" ~ "OH",
                   State.or.territory == "Idaho" ~ "ID",
                   State.or.territory == "Iowa" ~ "IA",
                   State.or.territory == "Kentucky" ~ "KY",
                   State.or.territory == "Utah" ~ "UT",
                   State.or.territory == "New Hampshire" ~ "NH",
                   State.or.territory == "Texas" ~ "TX",
                   State.or.territory == "Louisiana" ~ "LA",
                   State.or.territory == "South Dakota" ~ "SD",
                   State.or.territory == "North Dakota" ~ "ND",
                   State.or.territory == "New Mexico" ~ "NM",
                   State.or.territory == "Hawaii" ~ "HI",
                   State.or.territory == "North Carolina" ~ "NC",
                   State.or.territory == "South Carolina" ~ "SC",
                   State.or.territory == "Delaware" ~ "DE",
                   State.or.territory == "Illinois" ~ "IL",
                   State.or.territory == "Vermont" ~ "VT",
                   State.or.territory == "Colorado" ~ "CO",
                   State.or.territory == "New Jersey" ~ "NJ",
                   State.or.territory == "Montana" ~ "MT",
                   State.or.territory == "Missouri" ~ "MO",
                   State.or.territory == "Mississippi" ~ "MS",
                   State.or.territory == "Michigan" ~ "MI",
                   State.or.territory == "Wyoming" ~ "WY",
                   State.or.territory == "Oregon" ~ "OR",
                   State.or.territory == "West Virginia" ~ "WV",
                   State.or.territory == "Virginia" ~ "VA",
                   State.or.territory == "Arkansas" ~ "AR",
                   State.or.territory == "Pennsylvania" ~ "PA",
                   State.or.territory == "Connecticut" ~ "CT",
                   State.or.territory == "Kansas" ~ "KS",
                   State.or.territory == "Oklahoma" ~ "OK",
                   State.or.territory == "Maryland" ~ "MD",
                   State.or.territory == "Massachusetts" ~ "MA",
                   State.or.territory == "District of Columbia" ~ "DC",
                   State.or.territory == "Maine" ~ "ME",
                   State.or.territory == "Wisconsin" ~ "WI",
                   State.or.territory == "Nevada" ~ "NV",
                   State.or.territory == "Minnesota" ~ "MN",
                   State.or.territory == "New York" ~ "NY"))
```

```{r merge_state}
total_projs_by_state %>%
  left_join(successful_by_state) %>%
  left_join(census_population_df, by = c("location_state" = "st_code")) %>%
  mutate(prop_success_normalized = successful_projs / latest_pop) %>%
  ggplot() +
  geom_col(aes(x = fct_reorder(location_state, desc(prop_success_normalized)),
               y = prop_success_normalized,
               fill = location_state)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "State",
       y = "Number of Successful Projects per Resident",
       title = "Number of Successful Kickstarter Projects per Resident by State:\nDC, Vermont, Oregon & NY are Most Successful") +
  gghighlight(prop_success_normalized > 0.0005) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.0001))
  
```

#### Leaflet Map

I will consider "technology" to be a category that best indicates innovation, and then I will tally up the cities in Kickstarter campaigns.

```{r}
innovative_cities <- kickstarter %>%
  filter(top_category == "technology") %>%
  group_by(location_town, location_state) %>%
  count() %>%
  ungroup() %>%
  slice(-(1:55)) %>%
  arrange(desc(n)) %>%
  slice(1:50)
```

```{r, cache = TRUE, message = FALSE, warning=FALSE}
# use Google to find longitude and latitude coordinates for my most innovative cities
innovative_cities_new <- innovative_cities %>%
  mutate(lon = geocode(paste0(location_town, ", ", location_state))[1],
         lat = geocode(paste0(location_town, ", ", location_state))[2])

innovative_cities_clean <- innovative_cities_new %>%
  mutate(lon = as.numeric(pull(lon)),
         lat = as.numeric(pull(lat)))
```

```{r}
content <- paste("City:", innovative_cities_clean$location_town, "<br/>",
                 "State:", innovative_cities_clean$location_state, "<br/>",
                 "Number of Kickstarter Projects:", innovative_cities_clean$n, "<br/>")

pal <- colorFactor("Set1", domain = innovative_cities_clean$location_state)
color_offsel1 <- pal(innovative_cities_clean$location_state)

leaflet(as.data.frame(innovative_cities_clean)) %>%
  addTiles() %>%
  addCircleMarkers(color = color_offsel1,
             popup = content,
             clusterOptions = markerClusterOptions())
```


## 2. Writing your success story

### a) Cleaning the Text and Word Cloud

My metric for successful projects will be based on their achievement ratio.

```{r}
# filter on the 1000 most successful projects
successful <- kickstarter %>%
  select(name, blurb, pledged, goal) %>%
  mutate(achievement_ratio = pledged/goal * 100) %>%
  arrange(desc(achievement_ratio)) %>%
  slice_head(n = 1000)

# to define unsuccessful projects, I will filter on projects with $0 pledged in
# descending order of the goal amount (in other words, those with large goals and no funding)
# will be deemed most unsuccessful
unsuccessful <- kickstarter %>%
  select(name, blurb, pledged, goal) %>%
  filter(pledged <= 0) %>%
  arrange(desc(goal)) %>%
  slice_head(n = 1000)
```

Clean up words
```{r create_clean_function}
# create function to clean_text
clean_corpus <- function(input_corpus){
  
  new_corpus <- input_corpus %>%
    # prep data frame to convert into corpus
    mutate(doc_id = row_number()) %>%
    rename(text = blurb) %>%
    select(doc_id, text) %>%
    # convert into corpus
    DataframeSource() %>%
    VCorpus() %>%
    tm_map(content_transformer(function(x) iconv(x, to='UTF-8', sub='byte'))) %>%
    # clean corpus
    tm_map(removePunctuation) %>%          # remove punctuation
    tm_map(removeNumbers) %>%                  # remove numbers
    tm_map(stripWhitespace) %>%                # strip white space
    tm_map(content_transformer(tolower)) %>% #change to lower case
    tm_map(removeWords, c(stopwords("en")))  %>%  # remove English stop words
    tm_map(removeWords, c("rd", "st", "th", "squeeeeeee"))

  #### optional: convert into a tidy data frame ###
    #tidy() %>%
    #unnest_tokens(word, text) %>%
    #group_by(id, word) %>%
    return(new_corpus)
}
```

```{r apply_clean}
# apply function
successful_clean <- clean_corpus(successful)
unsuccessful_clean <- clean_corpus(unsuccessful)
```

Create DTM
```{r generate_dtm}
successful_clean_dtm <- DocumentTermMatrix(successful_clean)
unsuccessful_clean_dtm <- DocumentTermMatrix(unsuccessful_clean)

# preview DTM for successful projects
as.matrix(successful_clean_dtm)[100:103,1:5]
```

```{r create_wordcloud}
# calculate frequencies for the successful_clean_dtm

successful_tfidf <- successful_clean_dtm %>%
  tidy() %>%
  bind_tf_idf(term, document, count)

wordcloud(successful_tfidf$term,
          successful_tfidf$tf_idf,
          min.freq = 1,
          max.words = 60,
          colors = "magenta")
```
I used tf-idf as my metric to plot the words from the most successful projects.  This word cloud tells me that terms like "comic", "mantra, "lines", and "info" are popular and important words (based on rarer words and the frequency of the words) that appear in the Kickstarter blurbs.

### b) Success in Words

In this exercise, I will use term frequency as my metric for identifying the top terms.
```{r}
# create tf-idf tables for both the unsuccessful projects
unsuccessful_tfidf <- unsuccessful_clean_dtm %>%
  tidy() %>%
  group_by(term) %>%
  summarize(unsuccess_count = sum(count)) %>% # rename so we can differentiate it
  arrange(desc(unsuccess_count))
  
# create new data frame that is joined by the common words between the two data sets
# choose top 15 words based on the top 15 that show up overall based on the word count
top_15 <- successful_clean_dtm %>%
  tidy() %>%
  group_by(term) %>%
  summarize(success_count = sum(count)) %>%
  inner_join(unsuccessful_tfidf) %>%
  mutate(total_count = success_count + unsuccess_count) %>%
  arrange(desc(total_count)) %>%
  slice_head(n = 15) # take the top 15 terms

# generate pyramid plot
pyramid.plot(top_15$success_count,
             top_15$unsuccess_count,
             labels = top_15$term,
             gap = 10,
             laxlab = NULL,
             raxlab = NULL,
             unit = "Count of word",
             top.labels = c("Successful Projects", " ", "Unsuccessful Projects"),
             main = "Words in Common",
             labelcex = 0.8) +
  theme_fivethirtyeight()
  
```

### c) Simplicity as a virtue

```{r}

quanteda_flesch <- function (input_data) {
  
  new_corpus <- input_data %>%
    mutate(doc_id = row_number()) %>%
    rename(text = blurb) %>%
    select(doc_id, text) %>%
    # convert into corpus
    DataframeSource() %>%
    VCorpus()  %>%
    corpus() %>%# convert to quanteda corpus object
    textstat_readability(measure = c("Flesch.Kincaid"))
  
  return(new_corpus)
  }
```

```{r visualize_flesch}
# Create Quanteda Corpuses for both sets of data
qcorpus_success <- quanteda_flesch(successful)
qcorpus_unsuccess <- quanteda_flesch(unsuccessful)

# Row bind data and visualize
qcorpus_success %>%
  mutate(proj_type = "success") %>%
  cbind(select(successful, achievement_ratio)) %>%
  ggplot(aes(x = Flesch.Kincaid,
                 y = achievement_ratio)) +
  geom_point() +
  scale_color_viridis_d() +
  scale_y_log10() +
  geom_smooth(method = lm) +
  theme_bw() +
  labs(title = "Achievement Ratio vs Readability for Most Successful Kickstarters",
       x = "Flesch-Kincaid Readability Score",
       y = "Achievement Ratio as a % (Log-Transformed)")
```
This chart tells me that there is a decreasing, though weak, relationship between text complexity and fundraising attainment rates. In other words, texts with lower Flesch-Kincaid readability scores (which means they are more elementary and simple) tend to exceed their fundraising goals at a greater magnitude.

As an aside, there are negative Flesch-Kincaid scores because select blurbs use language that's relevant to their niche online communities (using poor grammar on purpose or incorporating words and special characters that are not recognized by the formal English language) and thus are not recognized as proper readability by the Flesch-Kincaid system.

## 3. Sentiment

### a) Stay positive

Set up dictionary and create function
```{r sentiment_dictionary}
# call sentiment dictionary
sentiment_dictionary <- tidytext::get_sentiments("bing")
```

```{r create_sentiment_function}

# create function to measure sentiment

evaluate_sentiment <- function(words){
  require(quanteda)
  words <- tolower(words)
  tok <- quanteda::tokens(words)
  pos.count <- sum(tok[[1]]
                   %in%
                     pull(select(filter(sentiment_dictionary,
                                        sentiment == "positive"), word)))
  neg.count <- sum(tok[[1]]
                   %in%
                     pull(select(filter(sentiment_dictionary,
                                        sentiment == "negative"),word)))
  out <- (pos.count - neg.count)/(pos.count+neg.count)
  return(out)
}
```

Evaluate sentiment for each blurb
```{r, cache = TRUE}
sentiment_output <- matrix(0, nrow = 1000, ncol = 1)

for (i in 1:nrow(successful)){
  sentiment_output[i,1] <- evaluate_sentiment(successful$blurb[i])}
```

Plot the results
```{r my_plot}
successful %>%
  cbind(sentiment_output) %>%
  mutate(sentiment_ouput = case_when(is.na(sentiment_output) == TRUE ~ 0,
                                     TRUE ~ as.numeric(sentiment_output))) %>%
  ggplot(aes(x = sentiment_output,
                   y = achievement_ratio)) +
  geom_point(aes(alpha = 0.01,
             color = cut_width(sentiment_output, 0.5))) +
  geom_jitter(aes(colour = cut_width(sentiment_output, 0.5))) +
  scale_color_viridis_d() +
  scale_y_log10() +
  theme_bw() +
  labs(title = "Achievement Ratio vs Sentiment Score for Most Successful Kickstarters",
       x = "Blurb Tone Score",
       y = "Achievement Ratio as a % (Log-Transformed)") +
  theme(legend.position = "none")
```
In this scatter plot, the majority of successful kickstarters use positive language, as demonstrated by the width and intense overlapping of the points in the "Blurb Tone Score" of 1.0 (which is a very positive score).  We can also observe that a significant number of successful campaigns' blurbs tend to have neutral sentiment (score = 0) or very negative sentiment (score = -1), and very few don't fit into these three buckets.

### b) Positive vs Negative

I will divide out the texts as negative and positive using the threshold that sentiment scores of 0 will be allocated to the positive bin.

```{r wordcloud2}
# create data frame that labels the pos/neg classification for each text
combined_df <- successful %>%
  cbind(sentiment_output) %>%
  mutate(sentiment_ouput = case_when(is.na(sentiment_output) == TRUE ~ 0,
                                     TRUE ~ as.numeric(sentiment_output))) %>%
  mutate(sentiment_label = ifelse(sentiment_ouput >= 0,
                                  "positive",
                                  "negative"))

# create a loop that will collapse the text into two groups

all_pos <- c()
all_neg <- c()

for (i in 1:nrow(combined_df)){
  
  if (combined_df$sentiment_label[i] == "positive"){
    all_pos <- paste(all_pos, combined_df$blurb[i])}
  
  if (combined_df$sentiment_label[i] == "negative"){
    all_neg <- paste(all_neg, combined_df$blurb[i])}
}

all_pos <- as.data.frame(all_pos) %>%
  mutate(type = "positive") %>%
  rename(blurb = all_pos)

all_neg <- as.data.frame(all_neg) %>%
  mutate(type = "negative") %>%
  rename(blurb = all_neg)

sentimentcorpus <- bind_rows(all_pos, all_neg) %>%
    mutate(doc_id = row_number()) %>%
    rename(text = blurb) %>%
    select(doc_id, text) %>%
    # convert into corpus
    DataframeSource() %>%
    VCorpus() %>%
    tm_map(content_transformer(function(x) iconv(x, to='UTF-8', sub='byte'))) %>%
    # clean corpus
    tm_map(removePunctuation) %>%          # remove punctuation
    tm_map(removeNumbers) %>%                  # remove numbers
    tm_map(stripWhitespace) %>%                # strip white space
    tm_map(content_transformer(tolower)) %>% #change to lower case
    tm_map(removeWords, c(stopwords("en")))  %>%  # remove English stop words
    tm_map(removeWords, c("rd", "st", "th", "squeeeeeee"))
```


```{r comparison_cloud, cache = TRUE}
combined_tfidf <- sentimentcorpus %>%
  DocumentTermMatrix() %>%
  t() %>%
  #tidy() %>%
  #bind_tf_idf(term, document, count) %>%
  as.matrix()

colnames(combined_tfidf) <- c("Most Freq Positive Words", "Most Freq Negative Words")

comparison.cloud(combined_tfidf,
                 color = c("blue", "red"),
                 scale = c(0.1, 1.9),
                 title.size = 1,
                 max.words = 60)
```

### c) Get in their mind

Set up dicitonary and create function to analyze text
```{r, cache = TRUE}
# extract NRC dictionary
nrc_dictionary <- tidytext::get_sentiments("nrc")

# write function to discern emotion
evaluate_sentiment_2 <- function(words){
  require(quanteda)
  words <- tolower(words)
  tok <- quanteda::tokens(words)
  
  trust <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "trust"), word)))
  
  fear <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "fear"),word)))
  
  negative <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "negative"), word)))
  
  sadness <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "sadness"),word)))
    
  anger <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "anger"), word)))
  
  surprise <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "surprise"),word)))
  
  positive <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "positive"), word)))
  
  disgust <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "disgust"),word)))
  
  joy <- sum(tok[[1]] %in% pull(select(filter(nrc_dictionary, sentiment == "joy"), word)))
  
  anticipation <- sum(tok[[1]] %in%
                        pull(select(filter(nrc_dictionary, sentiment == "anticipation"),word)))
  
  return(c(trust, anticipation, joy, disgust, positive, surprise, negative, anger, sadness, fear))
}
```


```{r, ache = TRUE}
sentiment_output_2 <- matrix(0, nrow = 1000, ncol = 10)
colnames(sentiment_output_2) <- c("trust", "anticipation", "joy", "disgust", "positive",
                                  "surprise", "negative", "anger", "sadness", "fear")

# loop through matrix and add emotions
for (i in 1:nrow(successful)){
  sentiment_output_2[i,] <- evaluate_sentiment_2(successful$blurb[i])}

```

Plot
```{r}
sentiment_output_2 %>%
  as.data.frame() %>%
  mutate(doc_id = row_number()) %>%
  bind_cols(select(successful, achievement_ratio)) %>%
  pivot_longer(trust:fear, names_to = "emotion", values_to = "count") %>%
  ggplot(aes(x = count, y = achievement_ratio)) +
  geom_point(aes(alpha = 0.01, color = emotion)) +
  geom_jitter(aes(alpha = 0.01, color = emotion)) +
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d() +
  scale_y_log10() +
  theme_classic() +
  labs(title = "Achievement Ratio vs Word-Emotion Use for Successful Kickstarters",
       x = "# of NRC Word-Emotion Terms in Blurb",
       y = "Achievement Ratio as a % (Log-Transformed)") +
  theme(legend.position = "none") +
  facet_wrap(~emotion)
  
```

My finding from visualizing various emotions to success (based on the metric of Achivement Ratio) is that there is no strong relationship between any specific NRC word-emotion.  However, there seems to be a weak decreasing relationship between the frequent use of "fear", "sadness", and "negative" words in the Kickstarter Blurb.  Meanwhile, there is a weak increasing relationship between the frequent use of words associated with "surprise", "anticipation", and "trust".  These trends are discerned by the linear model (lm) line that is drawn on each facet.